{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbvGCas9l1e+tfQk4XYaXI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitramir55/Tweet-Sentiment-Extraction/blob/main/TSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "V8I20uU6Utz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import re"
      ],
      "metadata": {
        "id": "Ve0eYAqGUtww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "2lSJGCN6Uym3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importing from Kaggle"
      ],
      "metadata": {
        "id": "cTQxoPsIWSou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle"
      ],
      "metadata": {
        "id": "gG9S17IaEDeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "xui9ixFjELgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "feae65ac-97f6-40bc-db74-8faff2c69fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0980a32e-436b-42db-ae12-4d77a96ce68f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0980a32e-436b-42db-ae12-4d77a96ce68f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mitramir5\",\"key\":\"f655276020b91f5e3f232e3c6d8e400d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create a directory for it\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4DyWCjRVZ_Q",
        "outputId": "b5a6f989-bf63-4bb9-921c-72025c5e6c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# change mode to full read and write access \n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "irWIB9QaVc7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c tweet-sentiment-extraction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Pun3IFVn_8",
        "outputId": "cc938a2a-5eba-49de-b5d7-ab36700cd228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading tweet-sentiment-extraction.zip to /content\n",
            "\r  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "\r100% 1.39M/1.39M [00:00<00:00, 152MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/tweet-sentiment-extraction.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzXk5DALV1zR",
        "outputId": "e1e8f6f8-6c10-417b-e6d9-9faca11d66b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/tweet-sentiment-extraction.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting with the dataset"
      ],
      "metadata": {
        "id": "zdjMgrzzWWmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "ZuMalqyoV6oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "m91IPXNlWjO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_train = len(df)\n",
        "len_test = len(df_test)\n",
        "\n",
        "print(f'dataset length is {len_train}')\n",
        "print(f'test dataset length is {len_test}')\n",
        "print(f'ratio = {round(len_test / len_train, 2)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKy_d0PBV_25",
        "outputId": "bb729307-68df-4ea2-ee22-f2df6004582a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset length is 27481\n",
            "test dataset length is 3534\n",
            "ratio = 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7a-gNPhEWbZD",
        "outputId": "db5b2431-e90a-4738-d098-f38f7e5f610b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       textID  \\\n",
              "0  cb774db0d1   \n",
              "1  549e992a42   \n",
              "2  088c60f138   \n",
              "3  9642c003ef   \n",
              "4  358bd9e861   \n",
              "\n",
              "                                                                          text  \\\n",
              "0                                          I`d have responded, if I were going   \n",
              "1                                Sooo SAD I will miss you here in San Diego!!!   \n",
              "2                                                    my boss is bullying me...   \n",
              "3                                               what interview! leave me alone   \n",
              "4   Sons of ****, why couldn`t they put them on the releases we already bought   \n",
              "\n",
              "                         selected_text sentiment  \n",
              "0  I`d have responded, if I were going   neutral  \n",
              "1                             Sooo SAD  negative  \n",
              "2                          bullying me  negative  \n",
              "3                       leave me alone  negative  \n",
              "4                        Sons of ****,  negative  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-420c257e-b912-4387-a754-4152b12797e1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-420c257e-b912-4387-a754-4152b12797e1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-420c257e-b912-4387-a754-4152b12797e1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-420c257e-b912-4387-a754-4152b12797e1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True, how='any')"
      ],
      "metadata": {
        "id": "effxqFvYgAX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbM81rybf6v6",
        "outputId": "0d5158a0-3fe9-4c7e-f7fb-4e70e960a1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "textID           0\n",
              "text             0\n",
              "selected_text    0\n",
              "sentiment        0\n",
              "jaccard_score    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_score(t1, t2):\n",
        "    \n",
        "    t1_words = set(t1.lower().split())\n",
        "    t2_words = set(t2.lower().split())\n",
        "    inter = t1_words.intersection(t2_words)\n",
        "    return len(inter) / (len(t1_words) + len(t2_words) - len(inter))"
      ],
      "metadata": {
        "id": "I4zrKJ-8h1GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.head(3):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrgUgOC1j7sE",
        "outputId": "92321d78-88c2-40b0-b3c7-01b6dfde967d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textID\n",
            "text\n",
            "selected_text\n",
            "sentiment\n",
            "jaccard_score\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:, 'jaccard_score'] = df.apply(lambda x: jaccard_score(x['selected_text'], x['text']), axis=1)"
      ],
      "metadata": {
        "id": "3HEYbeN-jDeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df.loc[:, 'selected_text']:\n",
        "    if re.findall('#', text): print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz-N03l3k0aF",
        "outputId": "504e0f98-96b9-4ae6-f0fc-02e38dd039e0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I still smell of smoke  #kitchenfire\n",
            "All the cool people I want to find for following today are #English, and I guess the English don`t tweet.\n",
            "good news: finally finished my #EASactive workout that has been paused for 6 hours. bad news: my resistance band is torn\n",
            "Simple my a#@\n",
            "It looks like the office TV DOES get MLB Network... and it looks like MLBN will NOT be televising the DET/BAL game today  #wieters\n",
            "there were attempts to somehow extend inner classes, which would be close to #closure, can`t find the ref ATM\n",
            "#stackeoverflow\n",
            "Goodmorning twitter, oh my gosh, i woke up soooo nice, lol ... oh hai thar twitterverse. Happy #mothersday everybody\n",
            "#bash\n",
            "Jaydiohead 'No Karma' ? http://twt.fm/91610 #musicmonday This is not the android I was looking for. But it`ll do.\n",
            "I don`t think I`ve ever been up this early on a Sunday in a while. Meanwhile, the #bigweekend awaits!\n",
            "WORS bike race at camp this weekend=a total of 1500 people. Going anywhere Sunday is not an option as they race right past the house  #fb\n",
            "I don`t have enough close contacts on twitter to do follow friday  #followfriday\n",
            "'The Complete Black Books' just arrived ... looking forward to some entertaining #tv nights!  #dvd\n",
            "I sent my donation to #Eric and wanted to put the banner on, but my avatar disappeared when I tried\n",
            "Anybody else think that #win7 is pretty much Vista SP2?\n",
            "YAY! Thanks for the #followfriday. #FAILFriday is still winning, though.\n",
            "Hours of refusal upon realisation that tomorrow`s #Morrissey gig at Brixton is postponed.   Only been looking fwd to that for 6 monthsish\n",
            "#youregreat\n",
            "Finished up cleaning my desk at least. . . office is looking empty. . .almost time to be in the car for 1 1/2!  #fb\n",
            "follow friday: following more people then followers  follow me? woot #followfriday\n",
            "Because of the new #Kirk. That`s why. Because of the new #Kirk.\n",
            "I WISH I could be in bed by nine once in a while. I never make it in before 3-4 a.m.  #sleepdeprived. Doing too much.\n",
            "seriously. increasingly good nights. stoked on life! couldn`t be happier  #goodnight\n",
            "tis` cool..I still stand by my former proclamation! I`m just stubborn that way! #rebellioustwitwhoknowsacoolcatcook lol\n",
            "Heading home from Sacramento.  Will continue writing the WIndows Server 2008 R2 Unleashed book over the weekend.  Sigh  #window\n",
            "#wheniwerealad can you explain how that worked? lol  and no rude  explanations pleeezzze\n",
            "#itsucks\n",
            "I`m glad/sad that it`s not just me/my iPhone  #badoptus\n",
            "Hey,  Thank you SO much for the #followfriday. I failed miserably to get a list together this week\n",
            "Get: 'Fightstar - Mercury Summer' here: http://tinyurl.com/dbm4n6 from: http://tinyurl.com/cnkhev  #musicmonday #freemusic\n",
            "In honour of #starwarsday, I have fired up a New Hope in the DVD player in full THX glory\n",
            "Now I need to find the Keynote one! At least I know where to go! #ScreenCastsOnline\n",
            "Had to cancel subscriptions today to NZZ and Economist at office  #costsavings Luckily they also have online versions\n",
            "Cleaning out my desk, I`ve found a betting slip from three months ago, I stuck a tenner on #newcastle getting relegated. In the money\n",
            "everyone get #buckonellen one of the topic things!\n",
            "#3wordsaftersex  Ow, rug burns\n",
            "I`m seeing quite a few proto-furries here, now that I think of it.  #acen\n",
            "off for a run. moon-day is not that bad after all...  #fb\n",
            "Hey #rda2009cla presentation was too large to post!  sorry\n",
            "More to thank for #ff shouts     and heaps more. Sry will be here all night i now realis\n",
            "Yeah, yeah. Less #degenerate than current occupants of U.S. House of Reprehensibles. You can bet on that. Like MineThatBird.\n",
            "@_refugee_ /me gets 'Your video will start in 15 seconds', Exiting to watch ... for minutes  #flash #linux #fail\n",
            "I would do a #FollowFriday, but everyone on my list is already on everyone elses list so what`s the point?  I`m just not popular enough.\n",
            "Looks like the #GM Bankruptcy is going to happen. This loss will help offset the gains that I had earlier in the year tax wise. Oh well\n",
            "-#FollowFriday is easy enough for those who are annoyed by it.  I recommend relatively few people anyway.\n",
            "bumped my domain zemote.com out of the #1 spot\n",
            "SMH @ PPL WHO ONLY TWIT U UP ON FRIDAYS B/C U GOT 100+ FOLLOWERS AND THEY`RE STILL BANKING 15-20  #NOFAKERY\n",
            "The dayem computer  #twpp\n",
            "Thus far, Metaverse U has used the Second Life = Virtual Worlds, rather than the gamier Terra Nova usage. I`m in the TN camp  #metaverseu\n",
            "You forgot moi  #twpp\n",
            "Good Morning All!   Hope everyone is doing well on this Monday!   Thanks for all the #followfriday recos!   I am bles\n",
            "is listening to music. really happy about !librefm #audacious combination\n",
            "1 week before the palm pre comes out and my centro dies. I have a temp phone but my contacts aren`t transfered yet. #sadpanda #fb\n",
            "#3wordsaftersex it never started...\n",
            "_chapman #whuffaoke is the same night as  at Gillette Stadium!\n",
            "kids out for summer/ pool has been taken over/ no more tanning girls  #haiku #yayschoolisout #sarcasm\n",
            "I don`t know enough about #Sotomayor, but she made a big boo-boo talking about making law.  Doesn`t know what 3 branches do?\n",
            "You mean set up to start using? just jump in! Use the # sign with whatever word you want to use\n",
            "goin to bed, but must say i love that #justin timberlake and #mark martin are the #3 and #4 trending topics on twitter tonite!  rock.\n",
            "_ Oh please mark all toe tweets with #toe.  is trying to make trening topic out of #toe\n",
            "#BGT DCD Seniors were good, but not good enough.\n",
            "#pens...steigy...no politics hun...the obamas are not watching hockey\n",
            "The pushing has started, not long before a new #starwarsday baby is born\n",
            "help! can`t even read the solutions online  #badmicrosoft\n",
            "#iusedtobescaredof\n",
            "I think #FF should be Feline Friday anyways, I want cat pictures. Maybe those are only for Caturday though\n",
            "#itsucks\n",
            "I have just looked the time and it is #BGT in 20mins. I may hibernate for an hour or so.\n",
            "Well, #Evernote is giving away Wireless SD cards 4 fllwng thm on twitter & tweeting abt #evernote_eyefi But US/CAN only  http://tr.im/mPxn\n",
            "By  Drove a #-mazda #RX8 today and boy, that car is fun! Sadly though, I don`t find my car that much fun any more\n",
            "just said goodbye to the younger bro.  #misshimalready\n",
            "So, #primavera is in Barcelona. In Spain. Surely Ruth Lorenzo must be here, right? BTW Jarvis has a walking stick\n",
            "#liesboystell Your the only one, I love (they really have several women)\n",
            "missed the #coffeeclub that day\n",
            "I think that`s cutting off the URL so the pic doesn`t turn up.   #mishaneedschapstick\n",
            "theres a holy version of hell? lol...i cant stop. i need for it to be a trending topic!  be nice (*whispering #redmango)lol\n",
            "I don`t think my friends like me anymore  (via #zenjar )\n",
            "ugh, i was so hungry i scarfed my lunch before i even thought to take a pic.   #feastfriday\n",
            "Boo for being at work during #beatwittyparty but at least you`re still here!\n",
            "Just dropped my sistah off @ lax.    #fb\n",
            "#fail\n",
            "I think i`m falling in love with you!!  **** #iloveyou\n",
            "Zelda Fans!!! pand_i - Zelda: The minnish cap, and a want for more music taste: Zelda = epic  I&#.. http://tinyurl.com/d7jvop\n",
            "Paul Scanlon sharing from God`s Word this morning, he`s sounding a bit hoarse perhaps a touch of *#cold*? It won`t hold our Pastor back\n",
            "thanks for the #ff shoutout -you stuck an extra e in my username tho, so the link doesn`t work\n",
            "two hours till our offer on the house expires  no word yet! Come on people, sell us your house!!!!!!!!!!! #Parker #Colorado\n",
            "wishing I was attending #beatweetup I even bought a badge\n",
            "fey slays it in #30rock but title of that film sounds bad  #Inbruges is a funny indie little euro city hitman gone bad tale!\n",
            "Bet she plays on Medium and you play on easy too huh...  #iPhone #Fun #Game\n",
            "#3wordsaftersex I haven`t started\n",
            "`m #frustrade\n",
            "*Hand up* Me, I`m going  #localgovcamp\n",
            "oh when will #gfail end? been on HTML version all day\n",
            "I know  I have no clean clothes either. And the washer`s in the kitchen  **** you #kitchenfire\n",
            "#shortcakefai\n",
            "#todo Cleaning the Apartment - again - who keeps making this mess? oh yeah .. me. $10 + hug for the person to help come clean\n",
            "No http://twtvite.com/3koyqo #twtvite #aptw Have to miss this now because of other engagements\n",
            "have you seen who`s knocked you off top spot on wefollow for #perth?\n",
            "Maybe if we pass a lay making the #swineflu illegal, law abiding citizens won`t get it\n",
            "#DuckRaces in the bath don`t quite work  Especially if you have no ducks!\n",
            "The book bloggers panel is not listed anywhere in the program!   Be there tomorrow at 2pm in room 1E15.  #BEA09\n",
            "bach I`d suggest #pta for such alerts, but PTA generally stands for Parent Teacher Association in the US & might confuse tweeps\n",
            "'Stop your twittering' - Maryland cookies #bgt\n",
            "You`ve been featured on #ykyat  http://ykyat.com/~37nnd\n",
            "John - more present than ever- even online  #edumedia09\n",
            "IDIOTat)tove_liden Th*nks for the follow Tove!  (ddoodm) #IDIOT\n",
            "Writing out to tape.  #48hoursnz\n",
            "#avatarcamp photos, perhaps?  lol (just itching to see them!)\n",
            "Poor Buddy, being teased by mini puggy  I think what you need is a #PUGHUG\n",
            "#Vancouver gets tough with slum landlords..ew backed up sewage\n",
            "schade  #tv_addict\n",
            "Appending the #verticalchinese hash tag is a nice touch.\n",
            "got an email from my auntie bout fathers day... mothers day w/seven moms but fathers day will consist of 2 dads    #realitychec\n",
            "I`m very sorry for the little disturbance at #SAPInsidetrack in Palo Alto! Just went into a debugging session when lines unmuted\n",
            "Cubbies at an even .500. So what else is new?  No matter: Beat dem Bums! #chicago #baseball\n",
            "Really! No even a little bit of #pane\n",
            "#sad\n",
            "I thought when u go #2 your tummy is supposed to feel better not worse\n",
            "Went to Di Bella coffee roasters today and asked for a job...   The person I needed to speak to is on holiday...  #f\n",
            "I love #polaroid - such a shame they don`t make the film for it anymore\n",
            "being a fan is certainly not about being attractive. But, if you like the pink, to each their own  #canucks\n",
            "I know.     #h\n",
            "It takes nearly 2 days to figure out what causes the mic problems at #euruko: Do not hold the mic at the bottom!\n",
            "getting impatient with the turnaround time on repairs to my (broken on arrival) new espresso machine  #fb\n",
            "That`s what I like to hear. I am unabashedly not cool. That`s what makes me so cool byw  #logic\n",
            "In rural #thailand, 2630 is considered posh\n",
            "More beans on toast for breakie! Gotta clean my room and study for final Cisco exam, then watch #LOST . Can`t wait!\n",
            "Have to wait till Friday to pick up my Visa for China- not as expedited as I would have hoped, but at it`s done   #geeksonaplane\n",
            "oof. perhaps it`s time to start a #moronmonday shout-out meme\n",
            "She lives!  You were so Twitter quiet after the race. Recovering or celebrating?  ps. #20 Fem. in your 1st marathon Wow!\n",
            "finally home and enjoying the rest of my day. Azongo and I are having a smoke-fest! woot! #mmot miss my\n",
            "_connors 1999 called, they want their saying back. Live in the NOW #pens  haha\n",
            "Boarding the USS Enterprise.. Warp speed ahead!   #fb\n",
            "live long and prosper  #fb\n",
            "Golden Girls marathon about to end  #lofnotc\n",
            "Checked the bank acct this mornin..got a lil sad..then decided I would imagine a 0 on the end just to make me smile.  #whateverworks\n",
            "So its Superstar Sunday?  is one superstar I know   #sunda\n",
            "Yeah, I made the python conversion script for the Provider`s extranet in 1 day (3 expected). A way to promote #python at #jcdecaux\n",
            "Loving the # dreambears\n",
            "_grubb Sweet... my friend Kevin is joining us (but I have his #). Look 4 the girl carrying a backpack & boulder pad.\n",
            "cool #movie\n",
            "Welcome  #Follow #Freude\n",
            "May the 4th be with you  #starwarsday (via )\n",
            "and now! ITS #starwarsday!!!! To celebrate i should watch some star wars\n",
            "_i_girl that`s why you are #twitterbff\n",
            "#fail\n",
            "#itsucks\n",
            "#flylady Oh deary me: 15 mins not enough to rescue kitchen  I shall come back to it later so I can mow the lawn while still cool outside\n",
            "_m except im on site near the airport on Dixon!  i have a lil #starbucks gps in my head hehe\n",
            "lamentablemente paso  #jrztwitterlunch\n",
            "Is anyone`s #visialvoicemail working. Have done the sync. Turned off & on. But still not going  Help?\n",
            "at the #manics gig.  Toasting your speedy recovery - sorry you can`t be here\n",
            "Hey #YEG !!!!! Anyone goin to the Edmonton Energy game and wanna do some live updates? PLEASE!!!! They dont post live scores\n",
            "thanks! I will try #boarding out\n",
            "#SanctuarySunday  yay for #Sanctuary, I may watch Requiem after breakfast!\n",
            "supposed to be back Sun (via private jet btw#!:o) but mite hav to stay bymyself (thnk god) for another week  & want Bruno!\n",
            "What about APE and server-side javascript with #mootools ? http://tinyurl.com/odqwgh  (via )\n",
            "#yourock\n",
            "ok its FF soooo why isn`t anyone following MEEEE??? LOL ughhhh I have some lazy a%# followers they wont even help me out\n",
            "there will be a session of #MoMoTLV at #iva09\n",
            "last #ff  _Diesel    _Web_Desig\n",
            "welcome to dk  #jaoo\n",
            "Wasn`t all bad #celtics won! But yeah, hope the #redsox do better. Good morning BTW.\n",
            "u drink mor than all th #g1freaks put togetha!\n",
            "twice in a week Qantas club lounge computers broken  #flyertalk\n",
            "cool #movie\n",
            "Graveyard charged my card twice (one correct total, one random amt) from last night`s #atltweet tweetup. Check your accounts!\n",
            "Happy Mother`s Day to all the moms! If you`re an awesome mom, you`ll help #savechuck\n",
            "the 14-24 is definitely going... sold the 70-200 and 200F2 is  too HEAVY  Think I`ll get away with the 50/85? #Tweddin\n",
            "has opted to strike, May 12th - Altaf bhai to unveil what really happened in #Karachi\n",
            "you should do an # for the safesex topic\n",
            "I`d rather be at #BEATweetup.  Instead, I`m taking my cue from  and shutting down.   reprezent-zen\n",
            "right, caffeine levels topped up, few more hours revision then #f1 time  `mon the jenson.\n",
            "Also, I am technologically challenged and have a cell phone that only makes phone calls.  Can`t #tweetb4Ueat\n",
            "Bought a new combo printer at costco today (officejet j4550) cause it was cheaper than ink 4 old one. #windows7 doesn`t like the fax part\n",
            "Boo  well its good fun tho.. sucks if u cant get cards.. I finished 10th in last nights #tpt I took  out LOL\n",
            "Ok, which album to start with? After listening to Frank Black for years I found out he used to play in #Pixies.\n",
            "well it is #stalkersaturday after all\n",
            "My apologies for the very impersonal #FF. Swamped today  All great people and great follows though. You can`t go wrong with those folks!\n",
            "... schedule says 10:30 class? I will be late for 10  #campjitterbug\n",
            "#Trackflashback: 'I Believe In A Thing Called Love' by The Darkness - check it out...\n",
            "Cops tell #Tori`s Dad they may never find her remains\n",
            "Drink #7 or 8 for me (at the club). Been drinking for the past 6 hours.  http://twitpic.com/4wn9q\n",
            "Happy #star wars day!\n",
            "Really good concepts at #mozconcept. I *really* love to send something mine,\n",
            "Wife & I split duties tonight.Wife headed to OSU for daughter`s ath trng rcption. I`m at son #1`s school paper banquet.Can`t make both.\n",
            "Go Canada they made Nikkie Payne #1 comic well she is soo yeah but she is only funny to pervs or teens or people in my family\n",
            "More #ecomonday   and me _fischer  Now I`ll go check out all the recommendations I`ve seen\n",
            "d happy mothers day!  love ya! #1 fan love you miley you rock\n",
            "put me right off my scrambled egg breakfast  #fartingloud\n",
            "My parents withdrew permission at the last moment, I am not going along with tomorrow`s SAC trip to IDP camps  #Pakistan #IDPRelief\n",
            "If there is one thing the Internet has taught me, is that if I get enough people to listen to me I get to be on The colbert report.  #USA\n",
            "nice work buying coda dude! It`s a great app! Unfortunately, I bought it when it was the full $99USD  #jealous\n",
            "You guys need to get on me. It`s been 2 weeks since I`ve went on a cache hunt  #geocaching\n",
            "#Volvicchallenge Tesco dropped off my 14 bottles today  Props for doing so on a Bank Holiday\n",
            "#FAIL\n",
            "Will arrive tomorrow morning at 7  #early\n",
            "Boys are sitting down for photos, no touching and no handshakes allowed.  #asylm\n",
            "no #thedailyshow this week\n",
            "#bgt Will Holly take being paraded on stage again for potential rejection?\n",
            "Getting ready to go to #Brighton to find myself a new suit from #GreshamBlake - Sunday = me time\n",
            "I`m sure the Mrs. will follow when she gets comfortable on the new computer, may be a while  #hhrs\n",
            "Har Har, #swineflu is everywhere: http://twitpic.com/4jb4o  (via )\n",
            "Hey, Wahts happening in #coffeclub..? I didnt have coffee for two days now\n",
            "is still working  #fb\n",
            "c`mon people, today is #juddday\n",
            "Oh, I is dubbed Breesaholic Insomniac #2  *feels special* oh! must add third Angel drink to post\n",
            "idk lol my head hurts. rly bad. & so does my stomach... hmm... personally, i think its from all the #s ive had 2 deal wit\n",
            "a bacon roll and a tea and the (hack)day is your friend  #openhacklondon\n",
            "#Java is not working - hmph!  Can`t upload photos to #Facebook.\n",
            "Jared`s wearing a green shirt? *SPARKLY EYES* YEE!! I hear #asylm is highly disorganized  hang in there!\n",
            "u  #awesomeupdater\n",
            "Just learned aobut #starwarsday. Thus: 'There are only 3 movies and Han Solo shot first'. Deal with it\n",
            "the #liesgirlstell and #liesboystell threads show how women and men alike are screwed up and struggle to have real, honest relationships\n",
            "#excited\n",
            "no one wants a #VirtualKiss\n",
            "#PhpEd 5.6 running successfuly via #wine , now to get EMS running\n",
            "Gain an instant follower, just use the #**** tag in a tweet. WTF? Lesson learned, Summer of Todd to get a new *official* tag.\n",
            "(Raises hand) Oh! Oh! Me too! #caffeine\n",
            "if you were down the road from me you can bet i would be right there  #MMOT\n",
            "Been working on a framework for web based #SL application. Have product registration, server, login, site, security, & comms all done\n",
            "i have perused the #fieldnotes website and it is good.  too bad i must return to work\n",
            "This is by far my #1 character flaw. Gotta session at 11 and I don`t want to emerge from my covers.\n",
            "#BGT Quite an awkward moment, poor girl  I hope she doesn`t loose it again.\n",
            "Going home... with sore #eyes\n",
            "Tshwane wants double rates but try and phone to get service - endless loop #fail\n",
            "missed the #jonaswebcast\n",
            "Wait...I thought it was 9:50? Don`t we have till 10?   #sigjeans\n",
            "OFF TOPIC: missed both motorcades. The secret service tricked us all.  #bush #clinton\n",
            "Got back and putting in the laundry. We got in there last  o well as long as its clean. #DSAA09\n",
            "Finally watched the last couple episodes of The Office on NBC ( #theofficenbc ) - I now understand why Gervais stopped after 2 seasons\n",
            "hehe indeed it is, I actually nearly changed my bio to just `Don`t Panic` but I wanted to keep in the #freehugs\n",
            "Oh dear  #bgt\n",
            "perhaps she will start including #magic in all of her tweets now too. BTW... you didn`t write #magic in ur tweet to me.\n",
            "#NHL Not a fan of either team, my head says Detroit, my feet say the Pens my heart doesn`t care   GO KINGS in 2009/2010\n",
            "#test SEO SMO marketing  try it\n",
            "you didn`t give out #followfriday advice like i told you?\n",
            "That`s exactly why I prefer to give money and play outside the house. Stupid Internet connection always ruining my instances!!!!  #wow\n",
            "Waiting for Raimi`s new horror flick Drag Me To Hell. I`m really excited for this! Too bad it`s not in #amcmain in KC\n",
            "#heartbreak\n",
            "None of their phone #`s work.\n",
            "I need some free #fieldnotes because I can`t travel to North 3rd Street in Philly to buy my own.\n",
            "Did 15 mins on expresso #cycling (3 miles) and beat my last time by 40 seconds.  Then lower body weights.  Now ready for work!\n",
            "How would we get thousand battlegrounds  #battleground, I mean we don`t even know how could we reach which number to make it trending tpc\n",
            "is #6 seed in regionals!!\n",
            "#sometimes also emoticons sometimes\n",
            "#myweakness too loving ...i always let negative people into my life\n",
            "I`m still waiting to find out what #caca stands for - hoping it`s not what I think it is since you #follow me for #caca.\n",
            "That`s more like it - 3rd  #fb\n",
            "#myweawkness chocolate..\n",
            "Aparantly it`s #starwarsday so enjoy  don`t quite know what your Kent to do but be happy anywho!\n",
            "why thx! #webdu I was only at the conf on Thurs\n",
            "IDIOTat)MilanQ Heyy. Th*nks For The Follow.  Have A Good Night. (Frank_Whyte) #IDIOT\n",
            "that`s cuz you`re cruising the twitter #nightshift now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "Yh3IzCI1gjPw",
        "outputId": "193cc3d1-c6c3-4548-c3ed-b9df62f42fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           textID  \\\n",
              "0      cb774db0d1   \n",
              "1      549e992a42   \n",
              "2      088c60f138   \n",
              "3      9642c003ef   \n",
              "4      358bd9e861   \n",
              "...           ...   \n",
              "27476  4eac33d1c0   \n",
              "27477  4f4c4fc327   \n",
              "27478  f67aae2310   \n",
              "27479  ed167662a5   \n",
              "27480  6f7127d9d7   \n",
              "\n",
              "                                                                                                                             text  \\\n",
              "0                                                                                             I`d have responded, if I were going   \n",
              "1                                                                                   Sooo SAD I will miss you here in San Diego!!!   \n",
              "2                                                                                                       my boss is bullying me...   \n",
              "3                                                                                                  what interview! leave me alone   \n",
              "4                                                      Sons of ****, why couldn`t they put them on the releases we already bought   \n",
              "...                                                                                                                           ...   \n",
              "27476                                                wish we could come see u on Denver  husband lost his job and can`t afford it   \n",
              "27477   I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet   \n",
              "27478              Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx   \n",
              "27479                                                                                                  But it was worth it  ****.   \n",
              "27480                                                                 All this flirting going on - The ATG smiles. Yay.  ((hugs))   \n",
              "\n",
              "                                                    selected_text sentiment  \\\n",
              "0                             I`d have responded, if I were going   neutral   \n",
              "1                                                        Sooo SAD  negative   \n",
              "2                                                     bullying me  negative   \n",
              "3                                                  leave me alone  negative   \n",
              "4                                                   Sons of ****,  negative   \n",
              "...                                                           ...       ...   \n",
              "27476                                                      d lost  negative   \n",
              "27477                                               , don`t force  negative   \n",
              "27478                                   Yay good for both of you.  positive   \n",
              "27479                                  But it was worth it  ****.  positive   \n",
              "27480  All this flirting going on - The ATG smiles. Yay.  ((hugs)   neutral   \n",
              "\n",
              "       jaccard_score  \n",
              "0           1.000000  \n",
              "1           0.200000  \n",
              "2           0.166667  \n",
              "3           0.600000  \n",
              "4           0.214286  \n",
              "...              ...  \n",
              "27476       0.058824  \n",
              "27477       0.083333  \n",
              "27478       0.272727  \n",
              "27479       1.000000  \n",
              "27480       0.833333  \n",
              "\n",
              "[27480 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-10858e7c-6dc0-4005-86a7-673b3f55d81d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>jaccard_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on the releases we already bought</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27476</th>\n",
              "      <td>4eac33d1c0</td>\n",
              "      <td>wish we could come see u on Denver  husband lost his job and can`t afford it</td>\n",
              "      <td>d lost</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27477</th>\n",
              "      <td>4f4c4fc327</td>\n",
              "      <td>I`ve wondered about rake to.  The client has made it clear .NET only, don`t force devs to learn a new lang  #agile #ccnet</td>\n",
              "      <td>, don`t force</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27478</th>\n",
              "      <td>f67aae2310</td>\n",
              "      <td>Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx</td>\n",
              "      <td>Yay good for both of you.</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27479</th>\n",
              "      <td>ed167662a5</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>But it was worth it  ****.</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27480</th>\n",
              "      <td>6f7127d9d7</td>\n",
              "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs))</td>\n",
              "      <td>All this flirting going on - The ATG smiles. Yay.  ((hugs)</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27480 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10858e7c-6dc0-4005-86a7-673b3f55d81d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10858e7c-6dc0-4005-86a7-673b3f55d81d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10858e7c-6dc0-4005-86a7-673b3f55d81d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len_train):\n",
        "    df.loc[i, 'jaccard_score'] = jaccard_score(df.loc[i, 'selected_text'], df.loc[i, 'text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "qIWGezoEgJIq",
        "outputId": "4672d7b6-9bd7-404c-db1b-004bed3ffdf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4f8b00ff220c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jaccard_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mjaccard_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \"\"\"\n\u001b[0;32m--> 790\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     MCM = multilabel_confusion_matrix(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [35, 36]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "f = lambda x: jaccard_score(x['selected_text'], x['text'])\n",
        "df.loc[:, 'jaccard_score'] = df.apply(lambda x: f(x), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "fiH_YPVuXOal",
        "outputId": "328733be-3dcb-4e3a-f30d-fbe50303594f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-e238925df355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jaccard_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-e238925df355>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jaccard_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-e238925df355>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'selected_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'jaccard_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mjaccard_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \"\"\"\n\u001b[0;32m--> 790\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m     \u001b[0msamplewise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     MCM = multilabel_confusion_matrix(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [35, 36]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:, 'jaccard_score'] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKxTT7uZa9Ft",
        "outputId": "9e9f26b9-434d-424d-8243-7b78b3392da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        <function <lambda> at 0x7f1c1892f950>\n",
              "1        <function <lambda> at 0x7f1c1892f950>\n",
              "2        <function <lambda> at 0x7f1c1892f950>\n",
              "3        <function <lambda> at 0x7f1c1892f950>\n",
              "4        <function <lambda> at 0x7f1c1892f950>\n",
              "                         ...                  \n",
              "27476    <function <lambda> at 0x7f1c1892f950>\n",
              "27477    <function <lambda> at 0x7f1c1892f950>\n",
              "27478    <function <lambda> at 0x7f1c1892f950>\n",
              "27479    <function <lambda> at 0x7f1c1892f950>\n",
              "27480    <function <lambda> at 0x7f1c1892f950>\n",
              "Name: jaccard_score, Length: 27481, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EdN35cA-dUdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}